Các bước cần làm 

Tạo 1 file .env để chứa key-ai

**Dowload model mi-mvc (git clone https://huggingface.co/nguyenvulebinh/vi-mrc-large)

Phải mô hình huấn luyện vi-mrc-lager từ hugging face



1. Tiếp nhận dữ liệu input từ người dùng

2. Xử lý dữ liệu bằng NLP (vi-mrc-large):
   - Tiền xử lý văn bản (chuẩn hóa, loại bỏ ký tự đặc biệt, v.v.)
   - Phân tích cú pháp và ngữ nghĩa
   - Trích xuất từ khóa và thông tin quan trọng

3. Detect-intent để phân loại yêu cầu thành 1 trong 4 hành động:
   - "qa_over_data" – Câu hỏi yêu cầu trả lời dựa trên dữ liệu
   - "data_query" – Truy vấn dữ liệu để hiển thị bảng kết quả
   - "generate_excel_report" – Xuất dữ liệu ra file Excel
   - "search_entity" – Truy tìm thông tin cụ thể trong dữ liệu

4. Xác định các tham số từ câu lệnh:
   - Nguồn dữ liệu (data_source)
   - Khoảng thời gian (fromtime, totime, thang, quy)
   - Các bộ lọc (chi nhánh, sản phẩm, v.v.)
   - Tên file xuất (nếu là generate_excel_report)
   - Nội dung câu hỏi (nếu là qa_over_data hoặc search_entity)

5. Tạo JSON với cấu trúc phù hợp:
   ```json
   {
     "action": "<loại hành động>",
     "data_source": "<nguồn dữ liệu>",
     "params": {
       "fromtime": "<dd/MM/yyyy>",
       "totime": "<dd/MM/yyyy>",
       "thang": <số>,
       "quy": <số>,
       "chi_nhanh": "<tên chi nhánh>",
       ...
     },
     "question": "<nội dung câu hỏi>",
     "output_name": "<tên file excel>"
   }
   ```

6. Chuyển kết quả JSON đến hệ thống xử lý tiếp theo

7. Xử lý và trả về kết quả theo yêu cầu người dùng

8. Lưu log và phản hồi để cải thiện hệ thống


======= HƯỚNG DẪN TRAINING MÔ HÌNH =======

## CHUẨN BỊ

1. Chuẩn bị môi trường:
   - Cài đặt Python 3.8+ và các thư viện: transformers, torch, pandas, sklearn
   - Cài đặt GPU drivers và CUDA toolkit (nếu có GPU)
   - Tạo môi trường ảo: `python -m venv nlp_training`
   - Kích hoạt môi trường: `source nlp_training/bin/activate` (Linux/Mac) hoặc `nlp_training\Scripts\activate` (Windows)

2. Chuẩn bị dữ liệu huấn luyện:
   - Thu thập ít nhất 500-1000 mẫu cho mỗi loại intent (qa_over_data, data_query, generate_excel_report, search_entity)
   - Định dạng dữ liệu theo cặp (câu hỏi, nhãn) trong file CSV/JSON
   - Chia tập dữ liệu: 80% training, 10% validation, 10% testing

## TIỀN XỬ LÝ DỮ LIỆU

3. Tiền xử lý dữ liệu:
   - Chuẩn hóa văn bản (loại bỏ dấu câu thừa, sửa lỗi chính tả)
   - Tạo bộ tokenizer cho tiếng Việt hoặc sử dụng tokenizer có sẵn từ PhoBERT/XLM-RoBERTa
   - Chuyển đổi dữ liệu thành định dạng tensor
   - Lưu trữ dữ liệu đã xử lý để tái sử dụng

## TRAINING MÔ HÌNH

4. Chuẩn bị mô hình cơ sở:
   - Tải mô hình cơ sở: `PhoBERT` hoặc `XLM-RoBERTa` hoặc `vi-mrc-large`
   - Cấu hình mô hình cho tác vụ phân loại hoặc trích xuất thông tin
   - Thiết lập lớp đầu ra phù hợp với số lượng nhãn

5. Training mô hình:
   - Thiết lập các siêu tham số: learning_rate=5e-5, batch_size=16, epochs=3-5
   - Sử dụng GPU nếu có để tăng tốc quá trình training
   - Theo dõi quá trình training bằng TensorBoard hoặc W&B
   - Lưu lại checkpoints sau mỗi epoch

6. Đánh giá và tối ưu hóa:
   - Đánh giá mô hình trên tập validation
   - Tinh chỉnh siêu tham số nếu cần
   - Áp dụng các kỹ thuật như early stopping, learning rate scheduling
   - Thử nghiệm các kiến trúc mô hình khác nhau

## TINH CHỈNH VÀ ĐÁNH GIÁ

7. Fine-tuning phân loại intent:
   - Train phần phân loại intent với 4 nhãn
   - Tối ưu các thông số F1-score, precision và recall
   - Kiểm tra các trường hợp phân loại sai và điều chỉnh

8. Fine-tuning trích xuất tham số:
   - Train mô hình trích xuất tham số từ câu hỏi
   - Tạo bộ dữ liệu có chú thích NER (Named Entity Recognition)
   - Đánh giá khả năng trích xuất các tham số như thời gian, chi nhánh, sản phẩm, v.v.

9. Đánh giá tổng thể:
   - Kiểm tra trên tập test
   - Đánh giá các metrics: accuracy, F1-score, precision, recall
   - Phân tích matrix nhầm lẫn (confusion matrix)
   - Xác định và khắc phục các lỗi phổ biến

## TÍCH HỢP VÀO HỆ THỐNG

10. Tối ưu hóa mô hình:
    - Giảm kích thước mô hình bằng distillation nếu cần
    - Chuyển đổi sang ONNX hoặc TorchScript để tăng tốc độ inference
    - Kiểm tra hiệu suất và thời gian phản hồi

11. Tích hợp vào API:
    - Tạo endpoint `/api/v1/nlp/train` để huấn luyện mô hình
    - Tạo endpoint `/api/v1/nlp/predict` để dự đoán intent và tham số
    - Lưu và tải mô hình từ ổ đĩa
    - Tạo cơ chế cập nhật mô hình mà không cần khởi động lại server

12. Giám sát và cải thiện liên tục:
    - Theo dõi hiệu suất mô hình trong thời gian thực
    - Thu thập feedback từ người dùng để cải thiện mô hình
    - Cập nhật mô hình định kỳ với dữ liệu mới
    - Tự động hóa quá trình re-training

## NÂNG CAO

13. Xây dựng pipeline huấn luyện tự động:
    - Tự động thu thập và xử lý dữ liệu mới
    - Pipeline CI/CD cho việc huấn luyện và triển khai mô hình
    - A/B testing với các phiên bản mô hình khác nhau

14. Tạo dashboard quản lý:
    - Hiển thị metrics của mô hình
    - Quản lý phiên bản mô hình
    - Giao diện để thêm dữ liệu huấn luyện mới
    - Công cụ gán nhãn dữ liệu (data labeling)

======= TRAINING MÔ HÌNH PHÂN LOẠI INTENT =======

## CẤU TRÚC THƯ MỤC CHO TRAINING

```
project/
├── app/
│   ├── models/                    # Thư mục chứa mô hình đã train
│   │   ├── intent_classifier/     # Mô hình phân loại intent
│   │   └── parameter_extractor/   # Mô hình trích xuất tham số
│   ├── routers/
│   │   ├── nlp.py                 # Router cho NLP API
│   │   └── ...
│   └── training/                  # Thư mục chứa code training
│       ├── data/                  # Dữ liệu huấn luyện
│       │   ├── raw/               # Dữ liệu thô
│       │   └── processed/         # Dữ liệu đã xử lý
│       ├── intent_classifier.py   # Script train phân loại intent
│       ├── parameter_extractor.py # Script train trích xuất tham số
│       └── utils.py               # Các hàm tiện ích
└── notebooks/                     # Jupyter notebooks cho phân tích
    ├── data_analysis.ipynb        # Phân tích dữ liệu
    └── model_evaluation.ipynb     # Đánh giá mô hình
```

## DỮ LIỆU MẪU CHO TRAINING

File CSV mẫu cho training intent (`app/training/data/raw/intent_samples.csv`):

```
query,intent
Cho tôi báo cáo doanh thu tháng 3,data_query
Xuất báo cáo Excel về doanh số chi nhánh Huế quý 2,generate_excel_report
Doanh thu của chi nhánh Đà Nẵng so với năm ngoái thế nào?,qa_over_data
Tìm thông tin khách hàng Nguyễn Văn A,search_entity
Kết quả kinh doanh quý 1 năm nay,data_query
Tạo file Excel thống kê nhân sự năm 2023,generate_excel_report
So sánh doanh số giữa chi nhánh Hà Nội và Hồ Chí Minh,qa_over_data
```

## SCRIPT TRAINING INTENT

1. Tạo file `app/training/intent_classifier.py`:

```python
import os
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Định nghĩa các intent
INTENT_LABELS = {
    "qa_over_data": 0,
    "data_query": 1, 
    "generate_excel_report": 2,
    "search_entity": 3
}

# Tokenizer và Model dựa trên PhoBERT hoặc vi-mrc-large
MODEL_NAME = "nguyenvulebinh/vi-mrc-large"  # hoặc "vinai/phobert-base"

class IntentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        inputs = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        return {
            "input_ids": inputs["input_ids"].flatten(),
            "attention_mask": inputs["attention_mask"].flatten(),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average="weighted"
    )
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

def train_intent_classifier():
    # Đường dẫn dữ liệu
    data_path = "app/training/data/raw/intent_samples.csv"
    output_dir = "app/models/intent_classifier"
    
    # Đảm bảo thư mục tồn tại
    os.makedirs(output_dir, exist_ok=True)
    
    # Đọc dữ liệu
    df = pd.read_csv(data_path)
    
    # Chuyển đổi intent sang số
    df["label"] = df["intent"].map(INTENT_LABELS)
    
    # Chia tập dữ liệu
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
    
    # Khởi tạo tokenizer và model
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME, num_labels=len(INTENT_LABELS)
    )
    
    # Tạo dataset
    train_dataset = IntentDataset(
        texts=train_df["query"].tolist(),
        labels=train_df["label"].tolist(),
        tokenizer=tokenizer
    )
    
    test_dataset = IntentDataset(
        texts=test_df["query"].tolist(),
        labels=test_df["label"].tolist(),
        tokenizer=tokenizer
    )
    
    # Thiết lập training args
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=5,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=10,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1",
    )
    
    # Khởi tạo trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
    )
    
    # Huấn luyện mô hình
    trainer.train()
    
    # Đánh giá mô hình
    results = trainer.evaluate()
    print(f"Evaluation results: {results}")
    
    # Lưu mô hình và tokenizer
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    print(f"Model saved to {output_dir}")
    
    return model, tokenizer

if __name__ == "__main__":
    train_intent_classifier()
```

## ROUTER NLP CHO FASTAPI

2. Tạo hoặc cập nhật file `app/routers/nlp.py`:

```python
from fastapi import APIRouter, HTTPException, UploadFile, File, Form, BackgroundTasks
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import torch
import os
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import pandas as pd

router = APIRouter()

# Định nghĩa các intent
INTENT_LABELS = {
    0: "qa_over_data",
    1: "data_query", 
    2: "generate_excel_report",
    3: "search_entity"
}

# Đường dẫn đến mô hình đã train
MODEL_DIR = "app/models/intent_classifier"

# Load mô hình
tokenizer = None
model = None

def load_model():
    global tokenizer, model
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
        model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
        model.eval()
        return True
    except Exception as e:
        print(f"Error loading model: {str(e)}")
        return False

# Hàm dự đoán intent
def predict_intent(text: str):
    if tokenizer is None or model is None:
        if not load_model():
            raise HTTPException(status_code=500, detail="Model not loaded")
    
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
    
    logits = outputs.logits
    predicted_class = torch.argmax(logits, dim=1).item()
    
    return {
        "intent": INTENT_LABELS[predicted_class],
        "confidence": torch.softmax(logits, dim=1)[0][predicted_class].item()
    }

# Class cho training data
class TrainingItem(BaseModel):
    query: str
    intent: str

class TrainingRequest(BaseModel):
    data: List[TrainingItem]

# API endpoints
@router.get("/status", summary="Trạng thái NLP", description="Kiểm tra trạng thái của mô hình NLP")
async def model_status():
    global model, tokenizer
    
    if model is None or tokenizer is None:
        is_loaded = load_model()
    else:
        is_loaded = True
    
    return {
        "model_loaded": is_loaded,
        "model_path": MODEL_DIR if is_loaded else None,
        "supported_intents": list(INTENT_LABELS.values())
    }

@router.post("/predict", summary="Dự đoán intent", description="Dự đoán intent từ câu query")
async def predict(query: str):
    result = predict_intent(query)
    
    # Giả lập trích xuất tham số (cần thay thế bằng mô hình trích xuất thực tế)
    params = {}
    
    if "tháng" in query.lower():
        for i in range(1, 13):
            if f"tháng {i}" in query.lower():
                params["thang"] = i
                break
    
    if "quý" in query.lower():
        for i in range(1, 5):
            if f"quý {i}" in query.lower():
                params["quy"] = i
                break
    
    if "chi nhánh" in query.lower():
        for city in ["Huế", "Đà Nẵng", "Hà Nội", "Hồ Chí Minh"]:
            if city in query:
                params["chi_nhanh"] = city
                break
    
    # Format kết quả theo yêu cầu
    return {
        "action": result["intent"],
        "confidence": result["confidence"],
        "params": params,
        "query": query
    }

@router.post("/train", summary="Train mô hình", description="Train lại mô hình với dữ liệu mới")
async def train_model(background_tasks: BackgroundTasks, request: TrainingRequest):
    # Lưu dữ liệu training
    training_data = pd.DataFrame([{"query": item.query, "intent": item.intent} for item in request.data])
    os.makedirs("app/training/data/raw", exist_ok=True)
    
    # Kiểm tra dữ liệu
    if len(training_data) < 10:
        raise HTTPException(status_code=400, detail="Cần ít nhất 10 mẫu để training")
    
    # Lưu dữ liệu mới
    training_data.to_csv("app/training/data/raw/intent_samples.csv", index=False)
    
    # Training trong background
    background_tasks.add_task(train_model_task)
    
    return {"status": "training_started", "message": "Đã bắt đầu quá trình training"}

async def train_model_task():
    # Import script training
    import sys
    sys.path.append("app/training")
    from intent_classifier import train_intent_classifier
    
    # Thực hiện training
    train_intent_classifier()

@router.post("/upload-training-data", summary="Upload dữ liệu training", 
             description="Upload file CSV chứa dữ liệu training")
async def upload_training_data(file: UploadFile = File(...)):
    # Kiểm tra định dạng file
    if not file.filename.endswith('.csv'):
        raise HTTPException(status_code=400, detail="Chỉ hỗ trợ file CSV")
    
    # Đọc file
    content = await file.read()
    
    # Lưu file
    os.makedirs("app/training/data/raw", exist_ok=True)
    with open("app/training/data/raw/intent_samples.csv", "wb") as f:
        f.write(content)
    
    return {"filename": file.filename, "message": "Upload thành công"}

# Load mô hình khi khởi động
@router.on_event("startup")
async def startup_db_client():
    load_model()
```

## SCRIPT CHẠY TRAINING

3. Tạo file `train_intent_model.py` trong thư mục gốc:

```python
import os
import sys

# Thêm đường dẫn dự án vào sys.path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import script training
from app.training.intent_classifier import train_intent_classifier

if __name__ == "__main__":
    print("Starting intent classification model training...")
    train_intent_classifier()
    print("Training completed!")
```

## HƯỚNG DẪN SỬ DỤNG

1. **Chuẩn bị dữ liệu**:
   - Tạo thư mục `app/training/data/raw`
   - Tạo file CSV chứa dữ liệu mẫu (xem mẫu ở trên)
   - Cần ít nhất 100 mẫu cho mỗi intent để đạt hiệu quả tốt

2. **Huấn luyện mô hình**:
   - Chạy lệnh: `python train_intent_model.py`
   - Hoặc sử dụng API: `POST /api/v1/nlp/train`

3. **Sử dụng mô hình**:
   - Gọi API: `POST /api/v1/nlp/predict?query=<câu_hỏi>`
   - Kết quả sẽ bao gồm intent dự đoán và các tham số trích xuất

4. **Bổ sung dữ liệu mới**:
   - Upload file CSV qua API: `POST /api/v1/nlp/upload-training-data`
   - Hoặc thêm trực tiếp vào file `app/training/data/raw/intent_samples.csv`

## YÊU CẦU VỀ THƯ VIỆN

Cài đặt các thư viện cần thiết:

```bash
pip install transformers torch pandas scikit-learn fastapi python-multipart
```

## CHÚ Ý

- Cần có ít nhất 500-1000 mẫu cho mỗi intent để đạt kết quả tốt
- Dữ liệu nên đa dạng và phản ánh các cách diễn đạt thực tế
- Nên sử dụng GPU để tăng tốc quá trình training
- Khi có dữ liệu mới, nên training lại mô hình để cải thiện hiệu suất

